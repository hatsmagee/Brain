# ğŸ§  Infinite Cartridge System

Brain-inspired modular neural network for Apple Silicon. Millions of tiny specialized "cartridges" (~100KB each) share frozen universal "stem" layers and differentiate via lightweight adapters.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INPUT QUERY                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STEM (Frozen Backbone)                     â”‚
â”‚   Universal layers trained once, shared by ALL cartridges   â”‚
â”‚   Like V1/A1 in cortex - early sensory processing           â”‚
â”‚   ~5MB, cached in SLC                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ROUTER (Prefrontal Cortex)                  â”‚
â”‚   Selects 2-4 relevant cartridges via learned signatures    â”‚
â”‚   Hebbian learning: successful paths strengthen             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚              â”‚              â”‚
       â–¼              â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cart A   â”‚   â”‚ Cart B   â”‚   â”‚ Cart C   â”‚   â”‚ Cart D   â”‚
â”‚ ~100KB   â”‚   â”‚ ~100KB   â”‚   â”‚ ~100KB   â”‚   â”‚ ~100KB   â”‚
â”‚ LoRA +   â”‚   â”‚ LoRA +   â”‚   â”‚ LoRA +   â”‚   â”‚ LoRA +   â”‚
â”‚ Signal   â”‚   â”‚ Signal   â”‚   â”‚ Signal   â”‚   â”‚ Signal   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GLOBAL WORKSPACE (Consciousness)               â”‚
â”‚   Top outputs compete for slots, winners broadcast to all   â”‚
â”‚   Like thalamo-cortical binding                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
                   OUTPUT
```

## Key Concepts

| Component | Brain Analog | Function |
|-----------|--------------|----------|
| Stem | V1, A1, early cortex | Universal feature extraction (frozen) |
| Cartridge | Provincial hub | Specialized domain expert |
| LoRA Adapter | Epigenetic marks | Lightweight specialization |
| Diff Signal | BMP/Wnt signaling | Domain identity vector |
| Router | Prefrontal cortex | Expert selection |
| Workspace | Global workspace | Conscious binding |
| Hebbian Update | Synaptic plasticity | "Fire together, wire together" |

## Memory Footprint

- **Stem (shared)**: ~5MB (loaded once)
- **Per cartridge**: ~100KB
- **Router + workspace**: ~500KB
- **Active set (4 carts)**: ~400KB

**1M cartridges = 100GB storage, but only ~6MB active at once**

Fits comfortably in M1 Max's 48MB SLC for streaming inference.

## Quick Start

```bash
# Install dependencies (macOS with Apple Silicon)
pip install mlx numpy fastapi uvicorn[standard] sse-starlette psutil

# Run
chmod +x run.sh
./run.sh

# Open dashboard
open http://localhost:8000
```

## Dashboard Features

- **Training Metrics**: Step, loss, active cartridge
- **Performance**: Tokens/sec, batch size, vocab size
- **System**: Memory, CPU usage
- **Chat**: Interactive generation
- **Visualization**: Radial cartridge graph
- **Registry**: All spawned cartridges

## Training Data

Put `.txt` files in `training_data/` directory. The system will:
1. Tokenize text
2. Build vocabulary
3. Spawn cartridges for token clusters
4. Train cartridges on their owned tokens

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Dashboard UI |
| `/stream/metrics` | GET (SSE) | Real-time metrics |
| `/cartridges` | GET | List all cartridges |
| `/training/pause` | POST | Pause training |
| `/training/resume` | POST | Resume training |
| `/training/mode/{cpu\|gpu}` | POST | Switch device |
| `/ws/chat` | WebSocket | Chat interface |

## Cartridge Lifecycle

1. **Spawn**: New domain detected â†’ cluster tokens â†’ create cartridge
2. **Train**: Receive batches â†’ update LoRA + head (stem frozen)
3. **Route**: Query arrives â†’ router scores signatures â†’ top-k selected
4. **Bind**: Outputs compete in workspace â†’ winners broadcast
5. **Hebbian**: Low loss â†’ strengthen connection; high loss â†’ weaken
6. **Save**: Periodic checkpoint to `cartridge_library/`

## Configuration

Edit `Config` class in `cartridge_system.py`:

```python
@dataclass 
class Config:
    dim: int = 256              # Model dimension
    stem_depth: int = 4         # Frozen backbone layers  
    adapter_rank: int = 16      # LoRA rank per cartridge
    cart_vocab: int = 1024      # Tokens per cartridge
    active_k: int = 4           # Active cartridges per query
    batch: int = 32
    seq_len: int = 64
    lr: float = 3e-4
```

## Scaling to Millions

```
cartridge_library/
â”œâ”€â”€ registry.json           # Token mappings
â”œâ”€â”€ domain_000/             # Science
â”‚   â”œâ”€â”€ cart_0000/
â”‚   â”‚   â”œâ”€â”€ weights.safetensors  (100KB)
â”‚   â”‚   â””â”€â”€ meta.json
â”‚   â”œâ”€â”€ cart_0001/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ domain_001/             # History
â””â”€â”€ ...

16 domains Ã— 256 topics Ã— 256 experts = 1M cartridges
```

Only active cartridges loaded; rest streamed on demand.

## License

MIT
